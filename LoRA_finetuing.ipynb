{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAmouVWSm9QX",
    "outputId": "22fe9e10-ec92-402b-8c4f-0ed2302093e0"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y unsloth transformers accelerate peft bitsandbytes\n",
    "!pip install -U pip\n",
    "!pip install -q \"unsloth[colab-new]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9In2P_fP5Br"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"train.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXUXNiAaQD1I"
   },
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"<s>[INST] {example['instruction']}\n",
    "\n",
    "{example['input']} [/INST]\n",
    "{example['output']}</s>\"\"\"\n",
    "    }\n",
    "dataset = dataset.map(format_example)\n",
    "dataset = dataset.remove_columns(\n",
    "    [col for col in dataset.column_names if col != \"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIH2JvVmPlCV",
    "outputId": "4080b267-e735-4570-962f-05234a38a280"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,      # IMPORTANT for T4\n",
    "    load_in_4bit=True,        # QLoRA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "595be12d39e941029604f1b10878c8d6",
      "b84fab91674344f492ce069c03b42d11",
      "a884cd565c684ce195d9b4769fd6b624",
      "3ee3e1d3276649ba87e2c7f854a059ec",
      "32b6ea954e9c414e947b5b96be946f01",
      "0299bf1467754112b3607392654120d6",
      "ff4d7c9bf4f74720af3a145076b8c04b",
      "b4faddab5e8c410f8209616391c60cc6",
      "56e80beffbe14d2eba7774809a481656",
      "8973ad323b1b4811b0dc054299142c7b",
      "1587b9035fae45afb36b863d205ac667"
     ]
    },
    "id": "Ukrhg-xgakXo",
    "outputId": "43cf4af9-e473-4d56-d598-9322a0e0fc3a"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=False,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "print(dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ETOOHCzXhgD",
    "outputId": "b9604fd3-e409-445f-c02e-f6af41c4d254"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrQhJ-u2Z_i0"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./unsloth-mistral-action\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # üîë FIX\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1TltoKcaCPd",
    "outputId": "a77ca1ae-d07c-48f1-cab5-6fdc7715ea41"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "YNBrosLQaEaV",
    "outputId": "86349731-1bf9-4e2b-d76b-6d06ba75cc4f"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thDH8DlSa9q0",
    "outputId": "f46065cf-5dbd-417b-d0ca-bad2ae002057"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"meeting-action-lora\")\n",
    "tokenizer.save_pretrained(\"meeting-action-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aQCUkHliA_S",
    "outputId": "9dd2ee12-4595-41aa-b428-70fa335a7ddc"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUL52Z_XiPH4",
    "outputId": "b6448906-503a-4476-be7a-13404a1cfef6"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"meeting-action-lora\",  # your saved LoRA directory\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xypVz6YWbBEA",
    "outputId": "b765c1dd-d153-4ca9-b903-172024031af1"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFyO435raGsO"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<s>[INST]\n",
    "You are a system that extracts action items from meeting transcripts.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY valid JSON\n",
    "- Do NOT add explanations, headings, or bullet points\n",
    "- Do NOT include text outside JSON\n",
    "- Use this schema exactly:\n",
    "\n",
    "{\n",
    "  \"action_items\": [\n",
    "    {\n",
    "      \"action\": string,\n",
    "      \"owner\": string | null,\n",
    "      \"deadline\": string | null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Meeting transcript:\n",
    "Alex: Please follow up with the client by Friday.\n",
    "Sam: I will send the email tomorrow.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.0,   # üîë critical\n",
    "    do_sample=False,  # üîë critical\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xutP8DqqaTld",
    "outputId": "11ef2d3c-ce43-4dc9-bd33-2b0da723af94"
   },
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove everything before the model answer\n",
    "answer = decoded.split('[/INST]')[-1].strip()\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Booy9VioRX2r"
   },
   "source": [
    "Schema Validity using Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ksqymShYXJW"
   },
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"eval.jsonl\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypTyNsDYbtK2"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError, ConfigDict\n",
    "from typing import List, Optional\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    action: str\n",
    "    owner: Optional[str]\n",
    "    deadline: Optional[str]\n",
    "\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "\n",
    "class ActionItemsOutput(BaseModel):\n",
    "    action_items: List[ActionItem]\n",
    "\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1SjZSq1dusE"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def extract_json(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1Ô∏è‚É£ Remove everything before [/INST]\n",
    "    if \"[/INST]\" in text:\n",
    "        text = text.split(\"[/INST]\", 1)[1].strip()\n",
    "\n",
    "    # 2Ô∏è‚É£ Try JSON\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3Ô∏è‚É£ Try Python literal (THIS is your case)\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgUY_v-efgoT"
   },
   "outputs": [],
   "source": [
    "def clean_action_items(data: dict) -> dict:\n",
    "    if \"action_items\" not in data:\n",
    "        return data\n",
    "\n",
    "    cleaned = []\n",
    "    for item in data[\"action_items\"]:\n",
    "        if isinstance(item, dict):\n",
    "            item = dict(item)\n",
    "            item.pop(\"action_items\", None)  # üîë REMOVE LEAK\n",
    "            cleaned.append(item)\n",
    "\n",
    "    data[\"action_items\"] = cleaned\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3tPirG-RUy_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def is_schema_valid(output):\n",
    "    try:\n",
    "        print(\"TYPE OF OUTPUT:\", type(output))\n",
    "\n",
    "        if isinstance(output, dict):\n",
    "            print(\"‚Üí Branch: dict\")\n",
    "            data = clean_action_items(output)\n",
    "            print(\"CLEANED DATA:\", data)\n",
    "            ActionItemsOutput(**data)\n",
    "            return True\n",
    "\n",
    "        if isinstance(output, str):\n",
    "            print(\"‚Üí Branch: str\")\n",
    "            data = extract_json(output)\n",
    "            print(\"EXTRACTED:\", data)\n",
    "            if data is None:\n",
    "                return False\n",
    "            data = clean_action_items(data)\n",
    "            print(\"CLEANED DATA:\", data)\n",
    "            ActionItemsOutput(**data)\n",
    "            return True\n",
    "\n",
    "        print(\"‚Üí Branch: neither\")\n",
    "        return False\n",
    "\n",
    "    except ValidationError as e:\n",
    "        print(\" VALIDATION ERROR:\", e)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c42_m6XRXGu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def run_inference(model, tokenizer, instruction, transcript):\n",
    "    prompt = f\"\"\"<s>[INST] {instruction}\n",
    "\n",
    "{transcript} [/INST]\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    data = extract_json(decoded)\n",
    "\n",
    "    if data is None:\n",
    "        print(\"‚ö†Ô∏è extract_json FAILED\")\n",
    "        return None\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZ1AXb1_Rimc"
   },
   "outputs": [],
   "source": [
    "def schema_validity_rate(model, tokenizer, eval_ds):\n",
    "    valid = 0\n",
    "\n",
    "    for ex in eval_ds:\n",
    "        output = run_inference(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            ex[\"instruction\"],\n",
    "            ex[\"input\"],\n",
    "        )\n",
    "\n",
    "        if is_schema_valid(output):\n",
    "            valid += 1\n",
    "\n",
    "    return valid / len(eval_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3P49bkxeAKJ",
    "outputId": "15657455-b99d-49e2-e0d4-bcbad4b5431d"
   },
   "outputs": [],
   "source": [
    "def debug_schema_examples(model, tokenizer, eval_ds, n=1):\n",
    "    for i in range(min(n, len(eval_ds))):\n",
    "        ex = eval_ds[i]\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Example {i+1}\")\n",
    "\n",
    "        print(\"\\nüìÑ TRANSCRIPT:\")\n",
    "        print(ex[\"input\"][:500], \"...\\n\")\n",
    "\n",
    "        output = run_inference(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            ex[\"instruction\"],\n",
    "            ex[\"input\"],\n",
    "        )\n",
    "\n",
    "        print(\"ü§ñ RAW MODEL OUTPUT:\")\n",
    "        print(output)\n",
    "\n",
    "        valid = is_schema_valid(output)\n",
    "        print(\"\\n‚úÖ SCHEMA VALID:\", valid)\n",
    "\n",
    "        # Optional: show parsed JSON if valid\n",
    "        if valid:\n",
    "            if isinstance(output, dict):\n",
    "                parsed = output\n",
    "            else:\n",
    "                parsed = extract_json(output)\n",
    "\n",
    "            print(\"\\nüß© PARSED JSON:\")\n",
    "            print(parsed)\n",
    "\n",
    "        print(\"\\n\")\n",
    "debug_schema_examples(model, tokenizer, eval_ds, n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WbgBxXkRkgo",
    "outputId": "f0ce33d3-3eea-4604-a997-2c723007b579"
   },
   "outputs": [],
   "source": [
    "rate = schema_validity_rate(model, tokenizer, eval_ds)\n",
    "print(f\"Schema validity: {rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5TGTa_dwQjM"
   },
   "source": [
    "Schema validity: 97.14%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "206dyrrvSORd"
   },
   "source": [
    "Prompt-only baseline (very important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hVczGEGSOp0",
    "outputId": "e4aee21b-80d5-4231-dea8-3714d0c36413"
   },
   "outputs": [],
   "source": [
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3GY177loNqL",
    "outputId": "c3e3956e-1a6d-4fec-bde4-96668458d94a"
   },
   "outputs": [],
   "source": [
    "rate = schema_validity_rate(base_model, base_tokenizer, eval_ds)\n",
    "print(f\"Schema validity: {rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYExTD8ewZs4"
   },
   "source": [
    "Schema validity: 0.00%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
